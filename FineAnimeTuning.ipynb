{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPyS/12XGUCPGFHltyTDyJk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "726a0b1038b049dfb251e543f226d014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17c0106d615243ea98aaca319b6d411e",
              "IPY_MODEL_314e3f0bd0b84fb69820ed701965daea",
              "IPY_MODEL_51bb5ca017c74e46af1acff54846a9a8"
            ],
            "layout": "IPY_MODEL_95caa824087447b4bd14521a3e134be4"
          }
        },
        "17c0106d615243ea98aaca319b6d411e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dd04187193048bd83cd7642974e3f33",
            "placeholder": "​",
            "style": "IPY_MODEL_8fcf499f03c14afc965098ab104ab16f",
            "value": ""
          }
        },
        "314e3f0bd0b84fb69820ed701965daea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8657f8f9faa7423886196bb36f953bb2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a35578097c2418893dce5b53b213865",
            "value": 0
          }
        },
        "51bb5ca017c74e46af1acff54846a9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27a641f0e8cf4f8b998340d98a36f71e",
            "placeholder": "​",
            "style": "IPY_MODEL_252b5b062aec4cabae57fe5220317649",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "95caa824087447b4bd14521a3e134be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd04187193048bd83cd7642974e3f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcf499f03c14afc965098ab104ab16f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8657f8f9faa7423886196bb36f953bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3a35578097c2418893dce5b53b213865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27a641f0e8cf4f8b998340d98a36f71e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252b5b062aec4cabae57fe5220317649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayanku1111/AnimagineXL_diffusion/blob/main/FineAnimeTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Zn1QE47u6Z",
        "outputId": "c8a18815-5dcb-4c5f-8e69-b5838f2615d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade diffusers invisible_watermark transformers accelerate safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnog5BF271HM",
        "outputId": "7529a676-9753-4175-b319-904265704b77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionXLPipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "726a0b1038b049dfb251e543f226d014",
            "17c0106d615243ea98aaca319b6d411e",
            "314e3f0bd0b84fb69820ed701965daea",
            "51bb5ca017c74e46af1acff54846a9a8",
            "95caa824087447b4bd14521a3e134be4",
            "8dd04187193048bd83cd7642974e3f33",
            "8fcf499f03c14afc965098ab104ab16f",
            "8657f8f9faa7423886196bb36f953bb2",
            "3a35578097c2418893dce5b53b213865",
            "27a641f0e8cf4f8b998340d98a36f71e",
            "252b5b062aec4cabae57fe5220317649"
          ]
        },
        "id": "VIV5ChIQ71Ko",
        "outputId": "665436db-6f1b-4465-ae96-0406ed77f03e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "726a0b1038b049dfb251e543f226d014"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mayanku1111/diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n_E2aOB71Nc",
        "outputId": "ffb231ca-a247-4c4a-8602-d368795c5907"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 50139, done.\u001b[K\n",
            "remote: Counting objects: 100% (10497/10497), done.\u001b[K\n",
            "remote: Compressing objects: 100% (957/957), done.\u001b[K\n",
            "remote: Total 50139 (delta 10116), reused 9589 (delta 9530), pack-reused 39642 (from 1)\u001b[K\n",
            "Receiving objects: 100% (50139/50139), 42.32 MiB | 12.32 MiB/s, done.\n",
            "Resolving deltas: 100% (37510/37510), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/diffusers/examples/dreambooth/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cbiQ4AJ71P3",
        "outputId": "db74f876-cbf4-48ae-c30c-3ee52d67ac42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/diffusers/examples/dreambooth/requirements.txt (line 1)) (0.33.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r /content/diffusers/examples/dreambooth/requirements.txt (line 2)) (0.18.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (4.44.0)\n",
            "Collecting ftfy (from -r /content/diffusers/examples/dreambooth/requirements.txt (line 4))\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (2.17.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r /content/diffusers/examples/dreambooth/requirements.txt (line 6)) (3.1.4)\n",
            "Collecting peft==0.7.0 (from -r /content/diffusers/examples/dreambooth/requirements.txt (line 7))\n",
            "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (4.66.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (0.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (0.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r /content/diffusers/examples/dreambooth/requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r /content/diffusers/examples/dreambooth/requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/diffusers/examples/dreambooth/requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r /content/diffusers/examples/dreambooth/requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r /content/diffusers/examples/dreambooth/requirements.txt (line 3)) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.7.0->-r /content/diffusers/examples/dreambooth/requirements.txt (line 7)) (1.3.0)\n",
            "Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, peft\n",
            "Successfully installed ftfy-6.2.3 peft-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers/examples/dreambooth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iw5KEsw71ST",
        "outputId": "c83cda63-ce4b-4840-ca5a-edfbf8947371"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers/examples/dreambooth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53pWrLwQ8Nq7",
        "outputId": "cffa00b0-8698-4ede-978e-aa679f2b634c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !accelerate launch train_dreambooth_lora.py \\\n",
        "#   --pretrained_model_name_or_path \"runwayml/stable-diffusion-v1-5\" \\\n",
        "#   --instance_data_dir \"/content/drive/MyDrive/Saitama\" \\\n",
        "#   --class_data_dir \"/content/drive/MyDrive/class_images3\" \\\n",
        "#   --output_dir \"/content/drive/MyDrive/Saitamadreambooth_model2\" \\\n",
        "#   --with_prior_preservation \\\n",
        "#   --prior_loss_weight=1.0 \\\n",
        "#   --instance_prompt \"Saitama_character\" \\\n",
        "#   --class_prompt \"bald anime man in hero costume\" \\\n",
        "#   --resolution=512 \\\n",
        "#   --train_batch_size=1 \\\n",
        "#   --gradient_accumulation_steps=1 \\\n",
        "#   --checkpointing_steps=100 \\\n",
        "#   --learning_rate=1e-4 \\\n",
        "#   --report_to=\"wandb\" \\\n",
        "#   --lr_scheduler=\"constant\" \\\n",
        "#   --lr_warmup_steps=0 \\\n",
        "#   --max_train_steps=500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa7XSb2UVl7K",
        "outputId": "a96a0e3b-59d3-4c38-ce45-dd03f5a13dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-08-13 11:13:42.504439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-13 11:13:42.523749: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-13 11:13:42.529698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-13 11:13:43.718919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/13/2024 11:13:45 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 4.11MB/s]\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]\n",
            "model.safetensors:   0% 0.00/492M [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   6% 31.5M/492M [00:00<00:01, 241MB/s]\u001b[A\n",
            "\n",
            "text_encoder/config.json: 100% 617/617 [00:00<00:00, 4.68MB/s]\n",
            "\n",
            "\n",
            "scheduler/scheduler_config.json: 100% 308/308 [00:00<00:00, 1.92MB/s]\n",
            "\n",
            "model.safetensors:  13% 62.9M/492M [00:00<00:01, 278MB/s]\u001b[A\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 806/806 [00:00<00:00, 6.15MB/s]\n",
            "\n",
            "\n",
            "tokenizer/vocab.json:   0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)ature_extractor/preprocessor_config.json: 100% 342/342 [00:00<00:00, 2.34MB/s]\n",
            "Fetching 13 files:   8% 1/13 [00:00<00:06,  1.99it/s]\n",
            "\n",
            "\n",
            "tokenizer/special_tokens_map.json: 100% 472/472 [00:00<00:00, 4.86MB/s]\n",
            "\n",
            "\n",
            "\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 71.9MB/s]\n",
            "\n",
            "model.safetensors:  19% 94.4M/492M [00:00<00:01, 291MB/s]\u001b[A\n",
            "model.safetensors:  26% 126M/492M [00:00<00:01, 296MB/s] \u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/3.44G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 31.5M/3.44G [00:00<00:12, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  34% 168M/492M [00:00<00:01, 300MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 31.5M/335M [00:00<00:01, 237MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "unet/config.json: 100% 743/743 [00:00<00:00, 5.12MB/s]\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 73.4M/3.44G [00:00<00:09, 353MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  40% 199M/492M [00:00<00:00, 301MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "vae/config.json: 100% 547/547 [00:00<00:00, 2.93MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 62.9M/335M [00:00<00:01, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  47% 231M/492M [00:00<00:00, 304MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 126M/3.44G [00:00<00:08, 392MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 105M/335M [00:00<00:00, 277MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  55% 273M/492M [00:00<00:00, 298MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 168M/3.44G [00:00<00:09, 354MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 147M/335M [00:00<00:00, 319MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  62% 304M/492M [00:01<00:00, 300MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 210M/3.44G [00:00<00:09, 354MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 189M/335M [00:00<00:00, 338MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  70% 346M/492M [00:01<00:00, 318MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 252M/3.44G [00:00<00:08, 360MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 1.13MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 231M/335M [00:00<00:00, 345MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 294M/3.44G [00:00<00:08, 363MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  79% 388M/492M [00:01<00:00, 304MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 336M/3.44G [00:00<00:08, 368MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  85% 419M/492M [00:01<00:00, 299MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 273M/335M [00:00<00:00, 257MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  92% 451M/492M [00:01<00:00, 278MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 304M/335M [00:01<00:00, 255MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 377M/3.44G [00:01<00:10, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 262MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 268MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors: 100% 492M/492M [00:01<00:00, 268MB/s]\n",
            "Fetching 13 files:  38% 5/13 [00:02<00:03,  2.41it/s]\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 440M/3.44G [00:01<00:14, 207MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 472M/3.44G [00:01<00:13, 222MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 524M/3.44G [00:01<00:10, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 577M/3.44G [00:01<00:08, 340MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 629M/3.44G [00:01<00:07, 383MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 682M/3.44G [00:02<00:06, 412MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 734M/3.44G [00:02<00:06, 415MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 786M/3.44G [00:02<00:06, 420MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 839M/3.44G [00:02<00:05, 434MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 891M/3.44G [00:02<00:05, 439MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  27% 944M/3.44G [00:02<00:05, 435MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  29% 996M/3.44G [00:02<00:05, 420MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 1.05G/3.44G [00:02<00:05, 426MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 1.10G/3.44G [00:03<00:05, 434MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 1.15G/3.44G [00:03<00:05, 448MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  35% 1.21G/3.44G [00:03<00:05, 428MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  37% 1.26G/3.44G [00:03<00:05, 420MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 1.31G/3.44G [00:03<00:04, 429MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  40% 1.36G/3.44G [00:03<00:04, 442MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 1.42G/3.44G [00:03<00:04, 431MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  43% 1.47G/3.44G [00:03<00:05, 370MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 1.52G/3.44G [00:04<00:04, 388MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  45% 1.56G/3.44G [00:04<00:05, 355MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 1.60G/3.44G [00:04<00:05, 329MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  48% 1.66G/3.44G [00:04<00:04, 360MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 1.71G/3.44G [00:04<00:04, 385MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  51% 1.75G/3.44G [00:04<00:04, 355MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  52% 1.79G/3.44G [00:04<00:05, 313MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 1.84G/3.44G [00:05<00:05, 313MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  55% 1.89G/3.44G [00:05<00:04, 355MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 1.94G/3.44G [00:05<00:03, 387MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  58% 1.99G/3.44G [00:05<00:03, 412MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 2.04G/3.44G [00:05<00:03, 426MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  61% 2.10G/3.44G [00:05<00:03, 437MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 2.15G/3.44G [00:05<00:03, 398MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 2.19G/3.44G [00:05<00:03, 390MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  65% 2.24G/3.44G [00:06<00:03, 348MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  67% 2.30G/3.44G [00:06<00:03, 375MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  68% 2.34G/3.44G [00:06<00:02, 375MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 2.38G/3.44G [00:06<00:02, 359MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  71% 2.43G/3.44G [00:06<00:02, 400MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 2.47G/3.44G [00:06<00:03, 304MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  74% 2.53G/3.44G [00:06<00:02, 337MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 2.58G/3.44G [00:07<00:02, 359MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  76% 2.62G/3.44G [00:07<00:02, 360MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 2.66G/3.44G [00:07<00:02, 369MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  79% 2.71G/3.44G [00:07<00:01, 375MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  80% 2.75G/3.44G [00:07<00:01, 378MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 2.79G/3.44G [00:07<00:01, 388MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  82% 2.83G/3.44G [00:07<00:01, 351MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  84% 2.87G/3.44G [00:07<00:01, 368MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 2.92G/3.44G [00:07<00:01, 339MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  86% 2.97G/3.44G [00:08<00:01, 366MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 3.01G/3.44G [00:08<00:01, 368MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  89% 3.06G/3.44G [00:08<00:01, 360MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  90% 3.10G/3.44G [00:08<00:00, 371MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 3.15G/3.44G [00:08<00:00, 378MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  93% 3.19G/3.44G [00:08<00:00, 371MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  94% 3.23G/3.44G [00:08<00:00, 378MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 3.27G/3.44G [00:08<00:00, 374MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  96% 3.31G/3.44G [00:09<00:00, 363MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  98% 3.36G/3.44G [00:09<00:00, 337MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  99% 3.40G/3.44G [00:09<00:00, 354MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:09<00:00, 367MB/s]\n",
            "Fetching 13 files: 100% 13/13 [00:10<00:00,  1.29it/s]\n",
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loading pipeline components...:  17% 1/6 [00:00<00:01,  4.21it/s]Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:01,  2.22it/s]Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'conv_out_kernel', 'timestep_post_act', 'time_embedding_act_fn', 'resnet_skip_time_act', 'mid_block_type', 'cross_attention_norm', 'num_attention_heads', 'resnet_out_scale_factor', 'upcast_attention', 'class_embed_type', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'time_embedding_dim', 'transformer_layers_per_block', 'attention_type', 'conv_in_kernel', 'class_embeddings_concat', 'time_embedding_type', 'addition_time_embed_dim', 'only_cross_attention', 'encoder_hid_dim_type', 'encoder_hid_dim', 'dropout', 'num_class_embeds', 'addition_embed_type_num_heads', 'time_cond_proj_dim', 'use_linear_projection', 'addition_embed_type', 'projection_class_embeddings_input_dim', 'dual_cross_attention', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loading pipeline components...:  83% 5/6 [00:02<00:00,  1.83it/s]{'latents_std', 'latents_mean', 'force_upcast', 'use_quant_conv', 'mid_block_add_attention', 'shift_factor', 'use_post_quant_conv', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loading pipeline components...: 100% 6/6 [00:02<00:00,  2.15it/s]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "08/13/2024 11:13:59 - INFO - __main__ - Number of class images to sample: 95.\n",
            "Generating class images: 100% 24/24 [04:40<00:00, 11.69s/it]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "{'thresholding', 'rescale_betas_zero_snr', 'sample_max_value', 'variance_type', 'prediction_type', 'dynamic_thresholding_ratio', 'clip_sample_range', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "{'latents_std', 'latents_mean', 'force_upcast', 'use_quant_conv', 'mid_block_add_attention', 'shift_factor', 'use_post_quant_conv', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "{'conv_out_kernel', 'timestep_post_act', 'time_embedding_act_fn', 'resnet_skip_time_act', 'mid_block_type', 'cross_attention_norm', 'num_attention_heads', 'resnet_out_scale_factor', 'upcast_attention', 'class_embed_type', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'time_embedding_dim', 'transformer_layers_per_block', 'attention_type', 'conv_in_kernel', 'class_embeddings_concat', 'time_embedding_type', 'addition_time_embed_dim', 'only_cross_attention', 'encoder_hid_dim_type', 'encoder_hid_dim', 'dropout', 'num_class_embeds', 'addition_embed_type_num_heads', 'time_cond_proj_dim', 'use_linear_projection', 'addition_embed_type', 'projection_class_embeddings_input_dim', 'dual_cross_attention', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "08/13/2024 11:20:42 - INFO - __main__ - ***** Running training *****\n",
            "08/13/2024 11:20:42 - INFO - __main__ -   Num examples = 100\n",
            "08/13/2024 11:20:42 - INFO - __main__ -   Num batches each epoch = 100\n",
            "08/13/2024 11:20:42 - INFO - __main__ -   Num Epochs = 5\n",
            "08/13/2024 11:20:42 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "08/13/2024 11:20:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "08/13/2024 11:20:42 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "08/13/2024 11:20:42 - INFO - __main__ -   Total optimization steps = 500\n",
            "Steps:  20% 100/500 [01:27<05:30,  1.21it/s, loss=0.0881, lr=0.0001]08/13/2024 11:22:09 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100\n",
            "Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100/pytorch_lora_weights.safetensors\n",
            "08/13/2024 11:22:10 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100/optimizer.bin\n",
            "08/13/2024 11:22:10 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100/scheduler.bin\n",
            "08/13/2024 11:22:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100/sampler.bin\n",
            "08/13/2024 11:22:10 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100/sampler_1.bin\n",
            "08/13/2024 11:22:10 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100/random_states_0.pkl\n",
            "08/13/2024 11:22:10 - INFO - __main__ - Saved state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-100\n",
            "Steps:  40% 200/500 [02:48<03:59,  1.25it/s, loss=0.0513, lr=0.0001]08/13/2024 11:23:31 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200\n",
            "Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200/pytorch_lora_weights.safetensors\n",
            "08/13/2024 11:23:31 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200/optimizer.bin\n",
            "08/13/2024 11:23:31 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200/scheduler.bin\n",
            "08/13/2024 11:23:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200/sampler.bin\n",
            "08/13/2024 11:23:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200/sampler_1.bin\n",
            "08/13/2024 11:23:31 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200/random_states_0.pkl\n",
            "08/13/2024 11:23:31 - INFO - __main__ - Saved state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-200\n",
            "Steps:  60% 300/500 [04:10<02:38,  1.26it/s, loss=0.38, lr=0.0001]08/13/2024 11:24:52 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300\n",
            "Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300/pytorch_lora_weights.safetensors\n",
            "08/13/2024 11:24:52 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300/optimizer.bin\n",
            "08/13/2024 11:24:53 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300/scheduler.bin\n",
            "08/13/2024 11:24:53 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300/sampler.bin\n",
            "08/13/2024 11:24:53 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300/sampler_1.bin\n",
            "08/13/2024 11:24:53 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300/random_states_0.pkl\n",
            "08/13/2024 11:24:53 - INFO - __main__ - Saved state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-300\n",
            "Steps:  80% 400/500 [05:31<01:14,  1.35it/s, loss=0.278, lr=0.0001]08/13/2024 11:26:14 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400\n",
            "Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400/pytorch_lora_weights.safetensors\n",
            "08/13/2024 11:26:14 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400/optimizer.bin\n",
            "08/13/2024 11:26:14 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400/scheduler.bin\n",
            "08/13/2024 11:26:14 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400/sampler.bin\n",
            "08/13/2024 11:26:14 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400/sampler_1.bin\n",
            "08/13/2024 11:26:14 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400/random_states_0.pkl\n",
            "08/13/2024 11:26:14 - INFO - __main__ - Saved state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-400\n",
            "Steps: 100% 500/500 [06:52<00:00,  1.29it/s, loss=0.421, lr=0.0001]08/13/2024 11:27:35 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500\n",
            "Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500/pytorch_lora_weights.safetensors\n",
            "08/13/2024 11:27:35 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500/optimizer.bin\n",
            "08/13/2024 11:27:35 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500/scheduler.bin\n",
            "08/13/2024 11:27:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500/sampler.bin\n",
            "08/13/2024 11:27:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500/sampler_1.bin\n",
            "08/13/2024 11:27:35 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500/random_states_0.pkl\n",
            "08/13/2024 11:27:35 - INFO - __main__ - Saved state to /content/drive/MyDrive/Saitamadreambooth_model2/checkpoint-500\n",
            "Steps: 100% 500/500 [06:53<00:00,  1.29it/s, loss=0.4, lr=0.0001]  Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model2/pytorch_lora_weights.safetensors\n",
            "\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "model.safetensors:   0% 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   2% 21.0M/1.22G [00:00<00:06, 181MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "safety_checker/config.json: 100% 4.72k/4.72k [00:00<00:00, 14.9MB/s]\n",
            "\n",
            "Fetching 15 files:  20% 3/15 [00:00<00:01,  6.01it/s]\u001b[A\n",
            "\n",
            "model.safetensors:   4% 52.4M/1.22G [00:00<00:05, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   7% 83.9M/1.22G [00:00<00:04, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   9% 115M/1.22G [00:00<00:04, 258MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  12% 147M/1.22G [00:00<00:04, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  15% 178M/1.22G [00:00<00:04, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  17% 210M/1.22G [00:00<00:04, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  20% 241M/1.22G [00:01<00:04, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  22% 273M/1.22G [00:01<00:04, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  25% 304M/1.22G [00:01<00:03, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  28% 336M/1.22G [00:01<00:03, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  30% 367M/1.22G [00:01<00:03, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  33% 398M/1.22G [00:01<00:03, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  35% 430M/1.22G [00:01<00:03, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  38% 461M/1.22G [00:01<00:03, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  41% 493M/1.22G [00:02<00:02, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  43% 524M/1.22G [00:02<00:02, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  46% 556M/1.22G [00:02<00:02, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  48% 587M/1.22G [00:02<00:02, 276MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  51% 619M/1.22G [00:02<00:02, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  53% 650M/1.22G [00:02<00:02, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  56% 682M/1.22G [00:02<00:02, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  59% 713M/1.22G [00:02<00:01, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  61% 744M/1.22G [00:03<00:01, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  64% 776M/1.22G [00:03<00:01, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  66% 807M/1.22G [00:03<00:01, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  69% 839M/1.22G [00:03<00:01, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  72% 870M/1.22G [00:03<00:01, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  74% 902M/1.22G [00:03<00:01, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  77% 933M/1.22G [00:03<00:01, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79% 965M/1.22G [00:03<00:00, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  82% 996M/1.22G [00:03<00:00, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85% 1.03G/1.22G [00:04<00:00, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87% 1.06G/1.22G [00:04<00:00, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  90% 1.09G/1.22G [00:04<00:00, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92% 1.12G/1.22G [00:04<00:00, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  95% 1.15G/1.22G [00:04<00:00, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  97% 1.18G/1.22G [00:04<00:00, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100% 1.22G/1.22G [00:04<00:00, 247MB/s]\n",
            "\n",
            "Fetching 15 files: 100% 15/15 [00:05<00:00,  2.88it/s]\n",
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  29% 2/7 [00:00<00:01,  4.75it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:00,  4.20it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'conv_out_kernel', 'timestep_post_act', 'time_embedding_act_fn', 'resnet_skip_time_act', 'mid_block_type', 'cross_attention_norm', 'num_attention_heads', 'resnet_out_scale_factor', 'upcast_attention', 'class_embed_type', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'time_embedding_dim', 'transformer_layers_per_block', 'attention_type', 'conv_in_kernel', 'class_embeddings_concat', 'time_embedding_type', 'addition_time_embed_dim', 'only_cross_attention', 'encoder_hid_dim_type', 'encoder_hid_dim', 'dropout', 'num_class_embeds', 'addition_embed_type_num_heads', 'time_cond_proj_dim', 'use_linear_projection', 'addition_embed_type', 'projection_class_embeddings_input_dim', 'dual_cross_attention', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  86% 6/7 [00:00<00:00,  8.28it/s]\u001b[A{'latents_std', 'latents_mean', 'force_upcast', 'use_quant_conv', 'mid_block_add_attention', 'shift_factor', 'use_post_quant_conv', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00,  7.63it/s]\n",
            "Loading unet.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss ▁▃▂▆▄▃▃▂▁▆▁▆█▁▅▁▁▄█▇▄▂▄▃▂▄▅▄▇▃▆▄▂▂▃▄▄▁▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.40004\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   lr 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/diffusers/examples/dreambooth/wandb/offline-run-20240813_112041-quj80uf6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20240813_112041-quj80uf6/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
            "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2333: UserWarning: Run (quj80uf6) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "Steps: 100% 500/500 [07:01<00:00,  1.19it/s, loss=0.4, lr=0.0001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "#   --pretrained_model_name_or_path \"Linaqruf/animagine-xl-3.0\" \\\n",
        "#   --instance_data_dir \"/content/drive/MyDrive/Saitama\" \\\n",
        "#   --class_data_dir \"/content/drive/MyDrive/class_images3\" \\\n",
        "#   --output_dir \"/content/drive/MyDrive/Saitamadreambooth_model3\" \\\n",
        "#   --with_prior_preservation \\\n",
        "#   --prior_loss_weight=1.0 \\\n",
        "#   --instance_prompt \"Saitama_character\" \\\n",
        "#   --class_prompt \"Saitama_character standing on a rooftop, cityscape background, dramatic lighting\" \\\n",
        "#   --resolution=512 \\\n",
        "#   --train_batch_size=1 \\\n",
        "#   --gradient_accumulation_steps=1 \\\n",
        "#   --checkpointing_steps=100 \\\n",
        "#   --learning_rate=1e-4 \\\n",
        "#   --lr_scheduler=\"constant\" \\\n",
        "#   --lr_warmup_steps=0 \\\n",
        "#   --max_train_steps=500"
      ],
      "metadata": {
        "id": "uKj_gsHQddSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path \"Linaqruf/animagine-xl-3.0\"  \\\n",
        "  --instance_data_dir \"/content/drive/MyDrive/Saitama\" \\\n",
        "  --output_dir \"/content/drive/MyDrive/Saitamadreambooth_model3\" \\\n",
        "  --mixed_precision \"fp16\" \\\n",
        "  --instance_prompt \"Saitama_character\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --report_to=\"wandb\" \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=500 \\\n",
        "  --validation_prompt \"Saitama casually walking through debris of a destroyed city\" \\\n",
        "  --validation_epochs=25 \\\n",
        "  --seed=\"0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMERUqzspHPK",
        "outputId": "d44ed24e-983b-44d4-c558-93db5ffa804a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-08-13 12:47:30.573686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-13 12:47:30.594192: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-13 12:47:30.600361: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-13 12:47:31.846360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/13/2024 12:47:33 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 704/704 [00:00<00:00, 4.85MB/s]\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 1.22MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 798kB/s]\n",
            "tokenizer/special_tokens_map.json: 100% 586/586 [00:00<00:00, 4.22MB/s]\n",
            "tokenizer_2/tokenizer_config.json: 100% 855/855 [00:00<00:00, 6.40MB/s]\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 3.38MB/s]\n",
            "text_encoder/config.json: 100% 560/560 [00:00<00:00, 4.10MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "text_encoder_2/config.json: 100% 570/570 [00:00<00:00, 4.02MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "model_index.json: 100% 577/577 [00:00<00:00, 3.87MB/s]\n",
            "scheduler/scheduler_config.json: 100% 474/474 [00:00<00:00, 3.40MB/s]\n",
            "{'variance_type', 'thresholding', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "model.safetensors: 100% 246M/246M [00:00<00:00, 273MB/s]\n",
            "model.safetensors: 100% 1.39G/1.39G [00:04<00:00, 297MB/s]\n",
            "vae/config.json: 100% 654/654 [00:00<00:00, 3.90MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 167M/167M [00:00<00:00, 359MB/s]\n",
            "{'mid_block_add_attention', 'latents_mean', 'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "unet/config.json: 100% 1.77k/1.77k [00:00<00:00, 11.3MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 5.14G/5.14G [00:22<00:00, 225MB/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "08/13/2024 12:48:46 - INFO - __main__ - ***** Running training *****\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Num examples = 7\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Num batches each epoch = 7\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Num Epochs = 250\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Total optimization steps = 500\n",
            "Steps:   0% 2/500 [00:08<35:01,  4.22s/it, loss=0.0782, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  57% 4/7 [00:00<00:00, 36.64it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 39.35it/s]\n",
            "08/13/2024 12:48:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  10% 52/500 [05:08<27:41,  3.71s/it, loss=0.0871, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 50.00it/s]\n",
            "08/13/2024 12:53:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  20% 102/500 [10:08<24:28,  3.69s/it, loss=0.0258, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.08it/s]\n",
            "08/13/2024 12:58:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  30% 152/500 [15:07<21:32,  3.71s/it, loss=0.251, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 55.19it/s]\n",
            "08/13/2024 13:03:57 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  40% 202/500 [20:08<18:27,  3.72s/it, loss=0.329, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 57.07it/s]\n",
            "08/13/2024 13:08:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  50% 252/500 [25:10<15:23,  3.72s/it, loss=0.226, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.71it/s]\n",
            "08/13/2024 13:13:59 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  60% 302/500 [30:11<12:20,  3.74s/it, loss=0.0597, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.41it/s]\n",
            "08/13/2024 13:19:01 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  70% 352/500 [35:12<09:05,  3.68s/it, loss=0.0267, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 57.51it/s]\n",
            "08/13/2024 13:24:02 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  80% 402/500 [40:12<06:02,  3.70s/it, loss=0.00349, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 57.72it/s]\n",
            "08/13/2024 13:29:02 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  90% 452/500 [45:12<02:57,  3.69s/it, loss=0.00284, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.26it/s]\n",
            "08/13/2024 13:34:02 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps: 100% 500/500 [50:04<00:00,  3.71s/it, loss=0.0867, lr=0.0001]08/13/2024 13:38:50 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500\n",
            "Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/pytorch_lora_weights.safetensors\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/optimizer.bin\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/scheduler.bin\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/sampler.bin\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Gradient scaler state saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/scaler.pt\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/random_states_0.pkl\n",
            "08/13/2024 13:38:51 - INFO - __main__ - Saved state to /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500\n",
            "Steps: 100% 500/500 [50:05<00:00,  3.71s/it, loss=0.0347, lr=0.0001]Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model3/pytorch_lora_weights.safetensors\n",
            "{'mid_block_add_attention', 'latents_mean', 'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  14% 1/7 [00:00<00:02,  2.98it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.72it/s]\u001b[A{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  5.37it/s]\n",
            "Loading unet.\n",
            "08/13/2024 13:38:54 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 1983, in <module>\n",
            "    main(args)\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 1950, in main\n",
            "    images = log_validation(\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 217, in log_validation\n",
            "    images = [pipeline(**pipeline_args, generator=generator).images[0] for _ in range(args.num_validation_images)]\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 217, in <listcomp>\n",
            "    images = [pipeline(**pipeline_args, generator=generator).images[0] for _ in range(args.num_validation_images)]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py\", line 1279, in __call__\n",
            "    image = self.vae.decode(latents, return_dict=False)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\n",
            "    return method(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\", line 321, in decode\n",
            "    decoded = self._decode(z).sample\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\", line 292, in _decode\n",
            "    dec = self.decoder(z)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/vae.py\", line 337, in forward\n",
            "    sample = up_block(sample, latent_embeds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_blocks.py\", line 2746, in forward\n",
            "    hidden_states = resnet(hidden_states, temb=temb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py\", line 327, in forward\n",
            "    hidden_states = self.norm1(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\", line 287, in forward\n",
            "    return F.group_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2588, in group_norm\n",
            "    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss ▃▇▁▅▁▅▁▅▁▆▂▁▄▁▄▂▆▂▁▁▁▁▁▁▁▁▃▄█▃▂▄▃█▇▂▁▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.0347\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   lr 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/diffusers/examples/dreambooth/wandb/offline-run-20240813_124845-q3ob6t4s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20240813_124845-q3ob6t4s/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 2 is less than current step: 3. Dropping entry: {'loss': 0.0015313893090933561, 'lr': 0.0001, '_timestamp': 1723553445.0265803}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 2 is less than current step: 3. Dropping entry: {'loss': 0.08717946708202362, 'lr': 0.0001, '_timestamp': 1723553446.0946743}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 2 is less than current step: 3. Dropping entry: {'loss': 0.22618253529071808, 'lr': 0.0001, '_timestamp': 1723553447.198994}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 52 is less than current step: 53. Dropping entry: {'loss': 0.1928362101316452, 'lr': 0.0001, '_timestamp': 1723553745.1949887}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 52 is less than current step: 53. Dropping entry: {'loss': 0.0029810487758368254, 'lr': 0.0001, '_timestamp': 1723553746.2884133}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 52 is less than current step: 53. Dropping entry: {'loss': 0.005535550881177187, 'lr': 0.0001, '_timestamp': 1723553747.3533978}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 102 is less than current step: 103. Dropping entry: {'loss': 0.11100862920284271, 'lr': 0.0001, '_timestamp': 1723554045.515621}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 102 is less than current step: 103. Dropping entry: {'loss': 0.009249312803149223, 'lr': 0.0001, '_timestamp': 1723554046.571108}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 102 is less than current step: 103. Dropping entry: {'loss': 0.11822101473808289, 'lr': 0.0001, '_timestamp': 1723554047.6237192}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 152 is less than current step: 153. Dropping entry: {'loss': 0.1634913980960846, 'lr': 0.0001, '_timestamp': 1723554345.2143214}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 152 is less than current step: 153. Dropping entry: {'loss': 0.013734008185565472, 'lr': 0.0001, '_timestamp': 1723554346.2791684}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 152 is less than current step: 153. Dropping entry: {'loss': 0.10733034461736679, 'lr': 0.0001, '_timestamp': 1723554347.3392832}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 202 is less than current step: 203. Dropping entry: {'loss': 0.014450730755925179, 'lr': 0.0001, '_timestamp': 1723554646.3293319}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 202 is less than current step: 203. Dropping entry: {'loss': 0.0030935383401811123, 'lr': 0.0001, '_timestamp': 1723554647.3874073}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 202 is less than current step: 203. Dropping entry: {'loss': 0.017319176346063614, 'lr': 0.0001, '_timestamp': 1723554648.4524775}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 252 is less than current step: 253. Dropping entry: {'loss': 0.19561266899108887, 'lr': 0.0001, '_timestamp': 1723554947.616901}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 252 is less than current step: 253. Dropping entry: {'loss': 0.1459880918264389, 'lr': 0.0001, '_timestamp': 1723554948.6762943}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 252 is less than current step: 253. Dropping entry: {'loss': 0.007808372378349304, 'lr': 0.0001, '_timestamp': 1723554949.7354448}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 302 is less than current step: 303. Dropping entry: {'loss': 0.011567145586013794, 'lr': 0.0001, '_timestamp': 1723555249.1062677}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 302 is less than current step: 303. Dropping entry: {'loss': 0.004249790217727423, 'lr': 0.0001, '_timestamp': 1723555250.1664824}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 302 is less than current step: 303. Dropping entry: {'loss': 0.009793409146368504, 'lr': 0.0001, '_timestamp': 1723555251.2522147}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 352 is less than current step: 353. Dropping entry: {'loss': 0.27281951904296875, 'lr': 0.0001, '_timestamp': 1723555550.1795614}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 352 is less than current step: 353. Dropping entry: {'loss': 0.024358363822102547, 'lr': 0.0001, '_timestamp': 1723555551.268477}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 352 is less than current step: 353. Dropping entry: {'loss': 0.07634267210960388, 'lr': 0.0001, '_timestamp': 1723555552.3273625}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 402 is less than current step: 403. Dropping entry: {'loss': 0.1369444578886032, 'lr': 0.0001, '_timestamp': 1723555849.9228034}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 402 is less than current step: 403. Dropping entry: {'loss': 0.00289700785651803, 'lr': 0.0001, '_timestamp': 1723555851.0003173}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 402 is less than current step: 403. Dropping entry: {'loss': 0.1151973307132721, 'lr': 0.0001, '_timestamp': 1723555852.07625}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 452 is less than current step: 453. Dropping entry: {'loss': 0.030292553827166557, 'lr': 0.0001, '_timestamp': 1723556149.7272437}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 452 is less than current step: 453. Dropping entry: {'loss': 0.030495157465338707, 'lr': 0.0001, '_timestamp': 1723556150.8137014}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 452 is less than current step: 453. Dropping entry: {'loss': 0.11430251598358154, 'lr': 0.0001, '_timestamp': 1723556151.880764}).\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1106, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 704, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'train_dreambooth_lora_sdxl.py', '--pretrained_model_name_or_path', 'Linaqruf/animagine-xl-3.0', '--instance_data_dir', '/content/drive/MyDrive/Saitama', '--output_dir', '/content/drive/MyDrive/Saitamadreambooth_model3', '--mixed_precision', 'fp16', '--instance_prompt', 'Saitama_character', '--resolution=1024', '--train_batch_size=1', '--gradient_accumulation_steps=4', '--learning_rate=1e-4', '--report_to=wandb', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--max_train_steps=500', '--validation_prompt', 'Saitama casually walking through debris of a destroyed city', '--validation_epochs=25', '--seed=0']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path \"Linaqruf/animagine-xl-3.0\"  \\\n",
        "  --instance_data_dir \"/content/drive/MyDrive/Cha Hae-in\" \\\n",
        "  --output_dir \"/content/drive/MyDrive/Cha-Hae-indreambooth_model\" \\\n",
        "  --mixed_precision \"fp16\" \\\n",
        "  --instance_prompt \"Cha_Hae_in\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=500 \\\n",
        "  --seed=\"0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpgKMC8edSoT",
        "outputId": "e7fa0178-2f24-45ff-a6e2-e2b2c2874e8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-08-14 15:24:48.137969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-14 15:24:48.157345: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-14 15:24:48.163515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-14 15:24:49.345150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/14/2024 15:24:51 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 704/704 [00:00<00:00, 4.87MB/s]\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 4.75MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 56.7MB/s]\n",
            "tokenizer/special_tokens_map.json: 100% 586/586 [00:00<00:00, 4.35MB/s]\n",
            "tokenizer_2/tokenizer_config.json: 100% 855/855 [00:00<00:00, 6.17MB/s]\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 3.59MB/s]\n",
            "text_encoder/config.json: 100% 560/560 [00:00<00:00, 4.22MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "text_encoder_2/config.json: 100% 570/570 [00:00<00:00, 3.83MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "model_index.json: 100% 577/577 [00:00<00:00, 3.84MB/s]\n",
            "scheduler/scheduler_config.json: 100% 474/474 [00:00<00:00, 3.30MB/s]\n",
            "{'clip_sample_range', 'thresholding', 'variance_type', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "model.safetensors: 100% 246M/246M [00:00<00:00, 370MB/s]\n",
            "model.safetensors: 100% 1.39G/1.39G [00:03<00:00, 401MB/s]\n",
            "vae/config.json: 100% 654/654 [00:00<00:00, 4.90MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 167M/167M [00:00<00:00, 407MB/s]\n",
            "{'latents_mean', 'use_post_quant_conv', 'mid_block_add_attention', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "unet/config.json: 100% 1.77k/1.77k [00:00<00:00, 12.8MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 5.14G/5.14G [00:15<00:00, 339MB/s] \n",
            "08/14/2024 15:25:42 - INFO - __main__ - ***** Running training *****\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Num examples = 6\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Num batches each epoch = 6\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Num Epochs = 250\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Total optimization steps = 500\n",
            "Steps: 100% 500/500 [26:47<00:00,  3.02s/it, loss=0.24, lr=0.0001]08/14/2024 15:52:30 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500\n",
            "Model weights saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/pytorch_lora_weights.safetensors\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/optimizer.bin\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/scheduler.bin\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/sampler.bin\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Gradient scaler state saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/scaler.pt\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/random_states_0.pkl\n",
            "08/14/2024 15:52:31 - INFO - __main__ - Saved state to /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500\n",
            "Steps: 100% 500/500 [26:48<00:00,  3.02s/it, loss=0.156, lr=0.0001]Model weights saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/pytorch_lora_weights.safetensors\n",
            "{'latents_mean', 'use_post_quant_conv', 'mid_block_add_attention', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "{'image_encoder', 'feature_extractor', 'add_watermarker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  14% 1/7 [00:00<00:01,  3.16it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.82it/s]\u001b[A{'timestep_type', 'final_sigmas_type', 'sigma_min', 'sigma_max', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.04it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  5.24it/s]\n",
            "Loading unet.\n",
            "Steps: 100% 500/500 [26:52<00:00,  3.22s/it, loss=0.156, lr=0.0001]\n"
          ]
        }
      ]
    }
  ]
}