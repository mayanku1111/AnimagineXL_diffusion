{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWm6lCKtWB5J2AFCyQ9crK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "726a0b1038b049dfb251e543f226d014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17c0106d615243ea98aaca319b6d411e",
              "IPY_MODEL_314e3f0bd0b84fb69820ed701965daea",
              "IPY_MODEL_51bb5ca017c74e46af1acff54846a9a8"
            ],
            "layout": "IPY_MODEL_95caa824087447b4bd14521a3e134be4"
          }
        },
        "17c0106d615243ea98aaca319b6d411e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dd04187193048bd83cd7642974e3f33",
            "placeholder": "​",
            "style": "IPY_MODEL_8fcf499f03c14afc965098ab104ab16f",
            "value": ""
          }
        },
        "314e3f0bd0b84fb69820ed701965daea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8657f8f9faa7423886196bb36f953bb2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a35578097c2418893dce5b53b213865",
            "value": 0
          }
        },
        "51bb5ca017c74e46af1acff54846a9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27a641f0e8cf4f8b998340d98a36f71e",
            "placeholder": "​",
            "style": "IPY_MODEL_252b5b062aec4cabae57fe5220317649",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "95caa824087447b4bd14521a3e134be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd04187193048bd83cd7642974e3f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcf499f03c14afc965098ab104ab16f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8657f8f9faa7423886196bb36f953bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3a35578097c2418893dce5b53b213865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27a641f0e8cf4f8b998340d98a36f71e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252b5b062aec4cabae57fe5220317649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayanku1111/AnimagineXL_diffusion/blob/main/FineAnimeTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Zn1QE47u6Z",
        "outputId": "c8a18815-5dcb-4c5f-8e69-b5838f2615d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade diffusers invisible_watermark transformers accelerate safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnog5BF271HM",
        "outputId": "7529a676-9753-4175-b319-904265704b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionXLPipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "726a0b1038b049dfb251e543f226d014",
            "17c0106d615243ea98aaca319b6d411e",
            "314e3f0bd0b84fb69820ed701965daea",
            "51bb5ca017c74e46af1acff54846a9a8",
            "95caa824087447b4bd14521a3e134be4",
            "8dd04187193048bd83cd7642974e3f33",
            "8fcf499f03c14afc965098ab104ab16f",
            "8657f8f9faa7423886196bb36f953bb2",
            "3a35578097c2418893dce5b53b213865",
            "27a641f0e8cf4f8b998340d98a36f71e",
            "252b5b062aec4cabae57fe5220317649"
          ]
        },
        "id": "VIV5ChIQ71Ko",
        "outputId": "665436db-6f1b-4465-ae96-0406ed77f03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "726a0b1038b049dfb251e543f226d014"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mayanku1111/diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n_E2aOB71Nc",
        "outputId": "ffb231ca-a247-4c4a-8602-d368795c5907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 50139, done.\u001b[K\n",
            "remote: Counting objects: 100% (10497/10497), done.\u001b[K\n",
            "remote: Compressing objects: 100% (957/957), done.\u001b[K\n",
            "remote: Total 50139 (delta 10116), reused 9589 (delta 9530), pack-reused 39642 (from 1)\u001b[K\n",
            "Receiving objects: 100% (50139/50139), 42.32 MiB | 12.32 MiB/s, done.\n",
            "Resolving deltas: 100% (37510/37510), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/diffusers/examples/dreambooth/requirements.txt"
      ],
      "metadata": {
        "id": "7cbiQ4AJ71P3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers/examples/dreambooth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iw5KEsw71ST",
        "outputId": "c83cda63-ce4b-4840-ca5a-edfbf8947371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers/examples/dreambooth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53pWrLwQ8Nq7",
        "outputId": "cffa00b0-8698-4ede-978e-aa679f2b634c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !accelerate launch train_dreambooth_lora.py \\\n",
        "#   --pretrained_model_name_or_path \"runwayml/stable-diffusion-v1-5\" \\\n",
        "#   --instance_data_dir \"/content/drive/MyDrive/Saitama\" \\\n",
        "#   --class_data_dir \"/content/drive/MyDrive/class_images3\" \\\n",
        "#   --output_dir \"/content/drive/MyDrive/Saitamadreambooth_model2\" \\\n",
        "#   --with_prior_preservation \\\n",
        "#   --prior_loss_weight=1.0 \\\n",
        "#   --instance_prompt \"Saitama_character\" \\\n",
        "#   --class_prompt \"bald anime man in hero costume\" \\\n",
        "#   --resolution=512 \\\n",
        "#   --train_batch_size=1 \\\n",
        "#   --gradient_accumulation_steps=1 \\\n",
        "#   --checkpointing_steps=100 \\\n",
        "#   --learning_rate=1e-4 \\\n",
        "#   --report_to=\"wandb\" \\\n",
        "#   --lr_scheduler=\"constant\" \\\n",
        "#   --lr_warmup_steps=0 \\\n",
        "#   --max_train_steps=500"
      ],
      "metadata": {
        "id": "aa7XSb2UVl7K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path \"Linaqruf/animagine-xl-3.0\"  \\\n",
        "  --instance_data_dir \"/content/drive/MyDrive/Saitama\" \\\n",
        "  --output_dir \"/content/drive/MyDrive/Saitamadreambooth_model3\" \\\n",
        "  --mixed_precision \"fp16\" \\\n",
        "  --instance_prompt \"Saitama_character\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --report_to=\"wandb\" \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=500 \\\n",
        "  --validation_prompt \"Saitama casually walking through debris of a destroyed city\" \\\n",
        "  --validation_epochs=25 \\\n",
        "  --seed=\"0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMERUqzspHPK",
        "outputId": "d44ed24e-983b-44d4-c558-93db5ffa804a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-08-13 12:47:30.573686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-13 12:47:30.594192: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-13 12:47:30.600361: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-13 12:47:31.846360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/13/2024 12:47:33 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 704/704 [00:00<00:00, 4.85MB/s]\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 1.22MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 798kB/s]\n",
            "tokenizer/special_tokens_map.json: 100% 586/586 [00:00<00:00, 4.22MB/s]\n",
            "tokenizer_2/tokenizer_config.json: 100% 855/855 [00:00<00:00, 6.40MB/s]\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 3.38MB/s]\n",
            "text_encoder/config.json: 100% 560/560 [00:00<00:00, 4.10MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "text_encoder_2/config.json: 100% 570/570 [00:00<00:00, 4.02MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "model_index.json: 100% 577/577 [00:00<00:00, 3.87MB/s]\n",
            "scheduler/scheduler_config.json: 100% 474/474 [00:00<00:00, 3.40MB/s]\n",
            "{'variance_type', 'thresholding', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "model.safetensors: 100% 246M/246M [00:00<00:00, 273MB/s]\n",
            "model.safetensors: 100% 1.39G/1.39G [00:04<00:00, 297MB/s]\n",
            "vae/config.json: 100% 654/654 [00:00<00:00, 3.90MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 167M/167M [00:00<00:00, 359MB/s]\n",
            "{'mid_block_add_attention', 'latents_mean', 'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "unet/config.json: 100% 1.77k/1.77k [00:00<00:00, 11.3MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 5.14G/5.14G [00:22<00:00, 225MB/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "08/13/2024 12:48:46 - INFO - __main__ - ***** Running training *****\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Num examples = 7\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Num batches each epoch = 7\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Num Epochs = 250\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
            "08/13/2024 12:48:46 - INFO - __main__ -   Total optimization steps = 500\n",
            "Steps:   0% 2/500 [00:08<35:01,  4.22s/it, loss=0.0782, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  57% 4/7 [00:00<00:00, 36.64it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 39.35it/s]\n",
            "08/13/2024 12:48:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  10% 52/500 [05:08<27:41,  3.71s/it, loss=0.0871, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 50.00it/s]\n",
            "08/13/2024 12:53:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  20% 102/500 [10:08<24:28,  3.69s/it, loss=0.0258, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.08it/s]\n",
            "08/13/2024 12:58:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  30% 152/500 [15:07<21:32,  3.71s/it, loss=0.251, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 55.19it/s]\n",
            "08/13/2024 13:03:57 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  40% 202/500 [20:08<18:27,  3.72s/it, loss=0.329, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 57.07it/s]\n",
            "08/13/2024 13:08:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  50% 252/500 [25:10<15:23,  3.72s/it, loss=0.226, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.71it/s]\n",
            "08/13/2024 13:13:59 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  60% 302/500 [30:11<12:20,  3.74s/it, loss=0.0597, lr=0.0001] {'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.41it/s]\n",
            "08/13/2024 13:19:01 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  70% 352/500 [35:12<09:05,  3.68s/it, loss=0.0267, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 57.51it/s]\n",
            "08/13/2024 13:24:02 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  80% 402/500 [40:12<06:02,  3.70s/it, loss=0.00349, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 57.72it/s]\n",
            "08/13/2024 13:29:02 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps:  90% 452/500 [45:12<02:57,  3.69s/it, loss=0.00284, lr=0.0001]{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 56.26it/s]\n",
            "08/13/2024 13:34:02 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Steps: 100% 500/500 [50:04<00:00,  3.71s/it, loss=0.0867, lr=0.0001]08/13/2024 13:38:50 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500\n",
            "Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/pytorch_lora_weights.safetensors\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/optimizer.bin\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/scheduler.bin\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/sampler.bin\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Gradient scaler state saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/scaler.pt\n",
            "08/13/2024 13:38:51 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500/random_states_0.pkl\n",
            "08/13/2024 13:38:51 - INFO - __main__ - Saved state to /content/drive/MyDrive/Saitamadreambooth_model3/checkpoint-500\n",
            "Steps: 100% 500/500 [50:05<00:00,  3.71s/it, loss=0.0347, lr=0.0001]Model weights saved in /content/drive/MyDrive/Saitamadreambooth_model3/pytorch_lora_weights.safetensors\n",
            "{'mid_block_add_attention', 'latents_mean', 'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "{'feature_extractor', 'add_watermarker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  14% 1/7 [00:00<00:02,  2.98it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.72it/s]\u001b[A{'sigma_max', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  5.37it/s]\n",
            "Loading unet.\n",
            "08/13/2024 13:38:54 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: Saitama casually walking through debris of a destroyed city.\n",
            "{'solver_order', 'lambda_min_clipped', 'solver_type', 'variance_type', 'thresholding', 'final_sigmas_type', 'lower_order_final', 'use_lu_lambdas', 'dynamic_thresholding_ratio', 'algorithm_type', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 1983, in <module>\n",
            "    main(args)\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 1950, in main\n",
            "    images = log_validation(\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 217, in log_validation\n",
            "    images = [pipeline(**pipeline_args, generator=generator).images[0] for _ in range(args.num_validation_images)]\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\", line 217, in <listcomp>\n",
            "    images = [pipeline(**pipeline_args, generator=generator).images[0] for _ in range(args.num_validation_images)]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py\", line 1279, in __call__\n",
            "    image = self.vae.decode(latents, return_dict=False)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\n",
            "    return method(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\", line 321, in decode\n",
            "    decoded = self._decode(z).sample\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\", line 292, in _decode\n",
            "    dec = self.decoder(z)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/vae.py\", line 337, in forward\n",
            "    sample = up_block(sample, latent_embeds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_blocks.py\", line 2746, in forward\n",
            "    hidden_states = resnet(hidden_states, temb=temb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py\", line 327, in forward\n",
            "    hidden_states = self.norm1(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\", line 287, in forward\n",
            "    return F.group_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2588, in group_norm\n",
            "    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss ▃▇▁▅▁▅▁▅▁▆▂▁▄▁▄▂▆▂▁▁▁▁▁▁▁▁▃▄█▃▂▄▃█▇▂▁▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.0347\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   lr 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/diffusers/examples/dreambooth/wandb/offline-run-20240813_124845-q3ob6t4s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20240813_124845-q3ob6t4s/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 2 is less than current step: 3. Dropping entry: {'loss': 0.0015313893090933561, 'lr': 0.0001, '_timestamp': 1723553445.0265803}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 2 is less than current step: 3. Dropping entry: {'loss': 0.08717946708202362, 'lr': 0.0001, '_timestamp': 1723553446.0946743}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 2 is less than current step: 3. Dropping entry: {'loss': 0.22618253529071808, 'lr': 0.0001, '_timestamp': 1723553447.198994}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 52 is less than current step: 53. Dropping entry: {'loss': 0.1928362101316452, 'lr': 0.0001, '_timestamp': 1723553745.1949887}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 52 is less than current step: 53. Dropping entry: {'loss': 0.0029810487758368254, 'lr': 0.0001, '_timestamp': 1723553746.2884133}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 52 is less than current step: 53. Dropping entry: {'loss': 0.005535550881177187, 'lr': 0.0001, '_timestamp': 1723553747.3533978}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 102 is less than current step: 103. Dropping entry: {'loss': 0.11100862920284271, 'lr': 0.0001, '_timestamp': 1723554045.515621}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 102 is less than current step: 103. Dropping entry: {'loss': 0.009249312803149223, 'lr': 0.0001, '_timestamp': 1723554046.571108}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 102 is less than current step: 103. Dropping entry: {'loss': 0.11822101473808289, 'lr': 0.0001, '_timestamp': 1723554047.6237192}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 152 is less than current step: 153. Dropping entry: {'loss': 0.1634913980960846, 'lr': 0.0001, '_timestamp': 1723554345.2143214}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 152 is less than current step: 153. Dropping entry: {'loss': 0.013734008185565472, 'lr': 0.0001, '_timestamp': 1723554346.2791684}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 152 is less than current step: 153. Dropping entry: {'loss': 0.10733034461736679, 'lr': 0.0001, '_timestamp': 1723554347.3392832}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 202 is less than current step: 203. Dropping entry: {'loss': 0.014450730755925179, 'lr': 0.0001, '_timestamp': 1723554646.3293319}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 202 is less than current step: 203. Dropping entry: {'loss': 0.0030935383401811123, 'lr': 0.0001, '_timestamp': 1723554647.3874073}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 202 is less than current step: 203. Dropping entry: {'loss': 0.017319176346063614, 'lr': 0.0001, '_timestamp': 1723554648.4524775}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 252 is less than current step: 253. Dropping entry: {'loss': 0.19561266899108887, 'lr': 0.0001, '_timestamp': 1723554947.616901}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 252 is less than current step: 253. Dropping entry: {'loss': 0.1459880918264389, 'lr': 0.0001, '_timestamp': 1723554948.6762943}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 252 is less than current step: 253. Dropping entry: {'loss': 0.007808372378349304, 'lr': 0.0001, '_timestamp': 1723554949.7354448}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 302 is less than current step: 303. Dropping entry: {'loss': 0.011567145586013794, 'lr': 0.0001, '_timestamp': 1723555249.1062677}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 302 is less than current step: 303. Dropping entry: {'loss': 0.004249790217727423, 'lr': 0.0001, '_timestamp': 1723555250.1664824}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 302 is less than current step: 303. Dropping entry: {'loss': 0.009793409146368504, 'lr': 0.0001, '_timestamp': 1723555251.2522147}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 352 is less than current step: 353. Dropping entry: {'loss': 0.27281951904296875, 'lr': 0.0001, '_timestamp': 1723555550.1795614}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 352 is less than current step: 353. Dropping entry: {'loss': 0.024358363822102547, 'lr': 0.0001, '_timestamp': 1723555551.268477}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 352 is less than current step: 353. Dropping entry: {'loss': 0.07634267210960388, 'lr': 0.0001, '_timestamp': 1723555552.3273625}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 402 is less than current step: 403. Dropping entry: {'loss': 0.1369444578886032, 'lr': 0.0001, '_timestamp': 1723555849.9228034}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 402 is less than current step: 403. Dropping entry: {'loss': 0.00289700785651803, 'lr': 0.0001, '_timestamp': 1723555851.0003173}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 402 is less than current step: 403. Dropping entry: {'loss': 0.1151973307132721, 'lr': 0.0001, '_timestamp': 1723555852.07625}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 452 is less than current step: 453. Dropping entry: {'loss': 0.030292553827166557, 'lr': 0.0001, '_timestamp': 1723556149.7272437}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 452 is less than current step: 453. Dropping entry: {'loss': 0.030495157465338707, 'lr': 0.0001, '_timestamp': 1723556150.8137014}).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 452 is less than current step: 453. Dropping entry: {'loss': 0.11430251598358154, 'lr': 0.0001, '_timestamp': 1723556151.880764}).\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1106, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 704, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'train_dreambooth_lora_sdxl.py', '--pretrained_model_name_or_path', 'Linaqruf/animagine-xl-3.0', '--instance_data_dir', '/content/drive/MyDrive/Saitama', '--output_dir', '/content/drive/MyDrive/Saitamadreambooth_model3', '--mixed_precision', 'fp16', '--instance_prompt', 'Saitama_character', '--resolution=1024', '--train_batch_size=1', '--gradient_accumulation_steps=4', '--learning_rate=1e-4', '--report_to=wandb', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--max_train_steps=500', '--validation_prompt', 'Saitama casually walking through debris of a destroyed city', '--validation_epochs=25', '--seed=0']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path \"Linaqruf/animagine-xl-3.0\"  \\\n",
        "  --instance_data_dir \"/content/drive/MyDrive/Cha Hae-in\" \\\n",
        "  --output_dir \"/content/drive/MyDrive/Cha-Hae-indreambooth_model\" \\\n",
        "  --mixed_precision \"fp16\" \\\n",
        "  --instance_prompt \"Cha_Hae_in\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=500 \\\n",
        "  --seed=\"0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpgKMC8edSoT",
        "outputId": "e7fa0178-2f24-45ff-a6e2-e2b2c2874e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-08-14 15:24:48.137969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-14 15:24:48.157345: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-14 15:24:48.163515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-14 15:24:49.345150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/14/2024 15:24:51 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 704/704 [00:00<00:00, 4.87MB/s]\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 4.75MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 56.7MB/s]\n",
            "tokenizer/special_tokens_map.json: 100% 586/586 [00:00<00:00, 4.35MB/s]\n",
            "tokenizer_2/tokenizer_config.json: 100% 855/855 [00:00<00:00, 6.17MB/s]\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 3.59MB/s]\n",
            "text_encoder/config.json: 100% 560/560 [00:00<00:00, 4.22MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "text_encoder_2/config.json: 100% 570/570 [00:00<00:00, 3.83MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "model_index.json: 100% 577/577 [00:00<00:00, 3.84MB/s]\n",
            "scheduler/scheduler_config.json: 100% 474/474 [00:00<00:00, 3.30MB/s]\n",
            "{'clip_sample_range', 'thresholding', 'variance_type', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "model.safetensors: 100% 246M/246M [00:00<00:00, 370MB/s]\n",
            "model.safetensors: 100% 1.39G/1.39G [00:03<00:00, 401MB/s]\n",
            "vae/config.json: 100% 654/654 [00:00<00:00, 4.90MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 167M/167M [00:00<00:00, 407MB/s]\n",
            "{'latents_mean', 'use_post_quant_conv', 'mid_block_add_attention', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "unet/config.json: 100% 1.77k/1.77k [00:00<00:00, 12.8MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 5.14G/5.14G [00:15<00:00, 339MB/s] \n",
            "08/14/2024 15:25:42 - INFO - __main__ - ***** Running training *****\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Num examples = 6\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Num batches each epoch = 6\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Num Epochs = 250\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
            "08/14/2024 15:25:42 - INFO - __main__ -   Total optimization steps = 500\n",
            "Steps: 100% 500/500 [26:47<00:00,  3.02s/it, loss=0.24, lr=0.0001]08/14/2024 15:52:30 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500\n",
            "Model weights saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/pytorch_lora_weights.safetensors\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/optimizer.bin\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/scheduler.bin\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/sampler.bin\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Gradient scaler state saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/scaler.pt\n",
            "08/14/2024 15:52:31 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500/random_states_0.pkl\n",
            "08/14/2024 15:52:31 - INFO - __main__ - Saved state to /content/drive/MyDrive/Cha-Hae-indreambooth_model/checkpoint-500\n",
            "Steps: 100% 500/500 [26:48<00:00,  3.02s/it, loss=0.156, lr=0.0001]Model weights saved in /content/drive/MyDrive/Cha-Hae-indreambooth_model/pytorch_lora_weights.safetensors\n",
            "{'latents_mean', 'use_post_quant_conv', 'mid_block_add_attention', 'shift_factor', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "{'image_encoder', 'feature_extractor', 'add_watermarker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  14% 1/7 [00:00<00:01,  3.16it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.82it/s]\u001b[A{'timestep_type', 'final_sigmas_type', 'sigma_min', 'sigma_max', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.04it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of Linaqruf/animagine-xl-3.0.\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  5.24it/s]\n",
            "Loading unet.\n",
            "Steps: 100% 500/500 [26:52<00:00,  3.22s/it, loss=0.156, lr=0.0001]\n"
          ]
        }
      ]
    }
  ]
}